{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9) AI-Assisted Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code has been generated by ChatGPT via the following prompt:\n",
    "\n",
    "*\"You are a python developer. Write a python script that implements the function multivariate_normal_density which takes the arguments x, mu, and Sigma. This function returns the density of a D-dimensional vector given a D-dimensional mean vector and a DxD-dimensional covariance matrix. The argument x is the D-dimensional input vector. The argument mu is the D-dimensional mean (location) vector.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density at [1 2]: 0.03836759318252469\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import inv, det\n",
    "\n",
    "def multivariate_normal_density(x, mu, Sigma):\n",
    "    \"\"\"\n",
    "    Calculate the density of a multivariate normal distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - x: D-dimensional input vector\n",
    "    - mu: D-dimensional mean vector\n",
    "    - Sigma: DxD-dimensional covariance matrix\n",
    "\n",
    "    Returns:\n",
    "    - Density of the multivariate normal distribution at the given input vector x.\n",
    "    \"\"\"\n",
    "    D = len(x)\n",
    "\n",
    "    if len(mu) != D or Sigma.shape != (D, D):\n",
    "        raise ValueError(\"Invalid dimensions for input vectors and matrices.\")\n",
    "\n",
    "    # Calculate the determinant and inverse of the covariance matrix\n",
    "    det_Sigma = det(Sigma)\n",
    "    inv_Sigma = inv(Sigma)\n",
    "\n",
    "    # Calculate the exponent term of the multivariate normal distribution\n",
    "    exponent_term = -0.5 * np.dot(np.dot((x - mu).T, inv_Sigma), (x - mu))\n",
    "\n",
    "    # Calculate the normalization constant\n",
    "    norm_constant = 1 / (np.sqrt((2 * np.pi) ** D * det_Sigma))\n",
    "\n",
    "    # Calculate the density\n",
    "    density = norm_constant * np.exp(exponent_term)\n",
    "\n",
    "    return density\n",
    "\n",
    "# Example usage:\n",
    "x = np.array([1, 2])\n",
    "mu = np.array([0, 0])\n",
    "Sigma = np.array([[1, 0.5], [0.5, 2]])\n",
    "\n",
    "result = multivariate_normal_density(x, mu, Sigma)\n",
    "print(f\"Density at {x}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the first run, ChatGPT seemingly has created well documented and somewhat logical code, including example usage. Below we will compare ChatGPT's implementation against Scipy's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT result: 0.03836759318252469\n",
      "Sci result: 0.03836759318252468\n",
      "Error: 1.808529888986545e-16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copying ChatGPT example\n",
    "x = np.array([1, 2])\n",
    "mu = np.array([0, 0])\n",
    "Sigma = np.array([[1, 0.5], [0.5, 2]])\n",
    "\n",
    "gpt_res = multivariate_normal_density(x, mu, Sigma)\n",
    "scipy_res = stats.multivariate_normal.pdf(x, mu, Sigma)\n",
    "\n",
    "print(f\"GPT result: {gpt_res}\\nSci result: {scipy_res}\")\n",
    "print(f\"Error: {abs(gpt_res-scipy_res)/scipy_res}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT result: 0.15915494309189535\n",
      "Sci result: 0.15915494309189535\n",
      "Error: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spherical Covariance\n",
    "x = np.array([0, 0])\n",
    "mu = np.array([0, 0])\n",
    "Sigma = np.eye(2)\n",
    "\n",
    "gpt_res = multivariate_normal_density(x, mu, Sigma)\n",
    "scipy_res = stats.multivariate_normal.pdf(x, mu, Sigma)\n",
    "\n",
    "print(f\"GPT result: {gpt_res}\\nSci result: {scipy_res}\")\n",
    "print(f\"Error: {abs(gpt_res-scipy_res)/scipy_res}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT result: 0.0711762543417177\n",
      "Sci result: 0.07117625434171772\n",
      "Error: 1.9497777645318473e-16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Diagonal Covariance\n",
    "x = np.array([0, 0])\n",
    "mu = np.array([0, 0])\n",
    "Sigma = np.array([[1.0, 0.0],[0.0, 5.0]])\n",
    "\n",
    "gpt_res = multivariate_normal_density(x, mu, Sigma)\n",
    "scipy_res = stats.multivariate_normal.pdf(x, mu, Sigma, allow_singular=True)\n",
    "\n",
    "print(f\"GPT result: {gpt_res}\\nSci result: {scipy_res}\")\n",
    "print(f\"Error: {abs(gpt_res-scipy_res)/scipy_res}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT result: 0.07302529613710933\n",
      "Sci result: 0.07302529613710936\n",
      "Error: 3.8008165777946537e-16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Full Covariance\n",
    "x = np.array([0, 0])\n",
    "mu = np.array([0, 0])\n",
    "Sigma = np.array([[1.0, 0.5],[0.5, 5.0]])\n",
    "\n",
    "gpt_res = multivariate_normal_density(x, mu, Sigma)\n",
    "scipy_res = stats.multivariate_normal.pdf(x, mu, Sigma, allow_singular=True)\n",
    "\n",
    "print(f\"GPT result: {gpt_res}\\nSci result: {scipy_res}\")\n",
    "print(f\"Error: {abs(gpt_res-scipy_res)/scipy_res}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:  \n",
    "For the simple example provided by GPT and the examples with Spherical, Diagonal, and Full Covariance, both of the algorithms practically matched each other and maintained a low error. While the GPT generation was considered a success in this case, developers should still carefully analyze the generated code to assert the correct functionality is being represented\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
